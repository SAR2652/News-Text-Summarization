{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1643909424879,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"pWPgHLPSRqYM","outputId":"6563c968-329e-497e-ec99-9098b4648143"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Feb  3 17:30:24 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19416,"status":"ok","timestamp":1643909444293,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"t5xW2dVmQzBV","outputId":"a84a61ef-a88f-4806-fbe6-777e1412e43c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3567,"status":"ok","timestamp":1643909447857,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"1doFTTNNYPcd","outputId":"a9993dd3-bc64-40ce-d9b8-a868872862d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 10.3 MB/s \n","\u001b[?25hCollecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 63.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.6.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.4.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.0.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets) (3.7.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.54.0)\n","Installing collected packages: tensorflow-addons, pickle5\n","Successfully installed pickle5-0.0.12 tensorflow-addons-0.15.0\n"]}],"source":["!pip install tqdm tensorflow_datasets tensorflow_addons pickle5"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"j4atwxEtZGtW","executionInfo":{"status":"ok","timestamp":1643909450829,"user_tz":300,"elapsed":2978,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["from tqdm import tqdm\n","import pickle5 as pickle\n","import pandas as pd\n","tqdm.pandas()\n","import os, string, re, json, ast\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from nltk import wordpunct_tokenize\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Layer, Input, Dense, Embedding, Concatenate, TimeDistributed, RNN, LSTMCell, AdditiveAttention, GRU, LSTM\n","from tensorflow.keras.backend import backend as K\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import Loss, SparseCategoricalCrossentropy\n","from tensorflow.keras.callbacks import EarlyStopping\n","import tensorflow_addons as tfa"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4-w8-pSCvPoJ","executionInfo":{"status":"ok","timestamp":1643909450830,"user_tz":300,"elapsed":7,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["filepath = '/content/drive/MyDrive/text_summarizer_models/no_glove/'"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"flPX0MC6azok","executionInfo":{"status":"ok","timestamp":1643909451761,"user_tz":300,"elapsed":937,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["tokenizer = pickle.load(open(filepath + 'tokenizer.pickle', 'rb'))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"L-Mu0QjX11xM","executionInfo":{"status":"ok","timestamp":1643909451762,"user_tz":300,"elapsed":9,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["vocab_size = len(tokenizer.word_index) + 1\n","max_features = vocab_size"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1643909451762,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"LIt5zu3TVVga","outputId":"cce9b50c-e6cf-4f74-8c37-4f3716ad5424"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["197747"]},"metadata":{},"execution_count":8}],"source":["vocab_size"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_e6OMGNjCELA","executionInfo":{"status":"ok","timestamp":1643909451763,"user_tz":300,"elapsed":5,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["# Create vocabulary using tokenizer's assigned indices to words\n","word_index = tokenizer.word_index"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"OzDlpw5HCcj3","executionInfo":{"status":"ok","timestamp":1643909451763,"user_tz":300,"elapsed":5,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["embed_size = 192\n","maxlen_article = 550\n","maxlen_summary = 50\n","batch_size = 32"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"2uU8QkK_CkAR","executionInfo":{"status":"ok","timestamp":1643909453901,"user_tz":300,"elapsed":2142,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["X_train = pickle.load(open(filepath + 'X_train.pickle', 'rb'))\n","y_train = pickle.load(open(filepath + 'y_train.pickle', 'rb'))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"6vi34upALqo_","executionInfo":{"status":"ok","timestamp":1643909454760,"user_tz":300,"elapsed":861,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["X_val = pickle.load(open(filepath + 'X_val.pickle', 'rb'))\n","y_val = pickle.load(open(filepath + 'y_val.pickle', 'rb'))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1643909454761,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"iFzrAuTLCp0F","outputId":"61134715-6722-45e1-c953-40da6c3589a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(79519, 550)"]},"metadata":{},"execution_count":13}],"source":["X_train.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1643909454762,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"ViLbgfVEMHbL","outputId":"5c49ab1c-6a39-47ec-b3d2-300b7c5ff839"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8836, 550)"]},"metadata":{},"execution_count":14}],"source":["X_val.shape"]},{"cell_type":"markdown","metadata":{"id":"nF3HLn4fhZlh"},"source":["## Text Summarizer Model"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MF_wQB9XAq-T","executionInfo":{"status":"ok","timestamp":1643909454762,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["class Encoder(Layer):\n","    def __init__(self, input_vocab_size, embedding_dim):\n","        super(Encoder, self).__init__()\n","        self.input_vocab_size = input_vocab_size\n","\n","        # The embedding layer converts tokens to vectors\n","        self.embedding = Embedding(self.input_vocab_size, embedding_dim, embeddings_initializer = 'glorot_normal')\n","\n","        # The GRU RNN layer processes those vectors sequentially.\n","\n","        self.lstm1 = LSTM(128, return_sequences=True, \n","                     return_state=True)\n","\n","        # self.lstm2 = LSTM(128, return_sequences=True,\n","        #              return_state=True)\n","    \n","        # self.lstm3 = LSTM(128, return_sequences=True,\n","        #              return_state=True)\n","\n","    def call(self, tokens, state=None):\n","    \n","        # 2. The embedding layer looks up the embedding for each token.\n","        vectors = self.embedding(tokens)\n","\n","        # 3. The GRU processes the embedding sequence.\n","        #    output shape: (batch, s, enc_units)\n","        #    state shape: (batch, enc_units)\n","        output_1, state_h1, state_c1  = self.lstm1(vectors)\n","        # output_2, state_h2, state_c2 = self.lstm2(output_1)\n","        # output_3, state_h3, state_c3 = self.lstm3(output_2)\n","        # 4. Returns the new sequence and its state.\n","        # return output_3, state_h3\n","        # return output_2, state_h2\n","        return output_1, state_h1"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"H3TNTnFm2ODS","executionInfo":{"status":"ok","timestamp":1643909454762,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["import typing\n","from typing import Any, Tuple\n","\n","class DecoderInput(typing.NamedTuple):\n","  new_tokens: Any\n","  enc_output: Any\n","  mask: Any\n","\n","class DecoderOutput(typing.NamedTuple):\n","  logits: Any\n","  attention_weights: Any"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"BJf5ZZ74gruU","executionInfo":{"status":"ok","timestamp":1643909454763,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super().__init__()\n","    # For Eqn. (4), the  Bahdanau attention\n","    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n","    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n","\n","    self.attention = AdditiveAttention()\n","\n","  def call(self, query, value, mask):\n","    # From Eqn. (4), `W1@ht`.\n","    w1_query = self.W1(query)\n","\n","    # From Eqn. (4), `W2@hs`.\n","    w2_key = self.W2(value)\n","\n","    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n","    value_mask = mask\n","\n","    context_vector, attention_weights = self.attention(\n","        inputs = [w1_query, value, w2_key],\n","        mask=[query_mask, value_mask],\n","        return_attention_scores = True,\n","    )\n","\n","    return context_vector, attention_weights"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"VxTgV-jYA3TJ","executionInfo":{"status":"ok","timestamp":1643909454763,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["class Decoder(Layer):\n","    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n","        super(Decoder, self).__init__()\n","        self.dec_units = dec_units\n","        self.output_vocab_size = output_vocab_size\n","        self.embedding_dim = embedding_dim\n","\n","        # For Step 1. The embedding layer converts token IDs to vectors\n","        self.embedding = Embedding(self.output_vocab_size, embedding_dim, embeddings_initializer = 'glorot_normal')\n","\n","        # For Step 2. The RNN keeps track of what's been generated so far.\n","        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","\n","        # For step 3. The RNN output will be the query for the attention layer.\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","        # For step 4. Eqn. (3): converting `ct` to `at`\n","        self.Wc = Dense(dec_units, activation=tf.keras.activations.tanh,\n","                                    use_bias=False)\n","\n","        # For step 5. This fully connected layer produces the logits for each\n","        # output token.\n","        self.fc = Dense(self.output_vocab_size)\n","    \n","    def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n","\n","        # Step 1. Lookup the embeddings\n","        vectors = self.embedding(inputs.new_tokens)\n","    \n","        # Step 2. Process one step with the RNN\n","        rnn_output, state, carry_state = self.lstm(vectors)\n","\n","        # Step 3. Use the RNN output as the query for the attention over the\n","        # encoder output.\n","        context_vector, attention_weights = self.attention(query = rnn_output, value = inputs.enc_output, mask = inputs.mask)\n","\n","        # Step 4. Eqn. (3): Join the context_vector and rnn_output\n","        #     [ct; ht] shape: (batch t, value_units + query_units)\n","        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis = -1)\n","\n","        # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n","        attention_vector = self.Wc(context_and_rnn_output)\n","        \n","        # Step 5. Generate logit predictions:\n","        logits = self.fc(attention_vector)\n","\n","        return DecoderOutput(logits, attention_weights), state"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7D0DJW5Lts07","executionInfo":{"status":"ok","timestamp":1643909454763,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["class MaskedLoss(Loss):\n","    def __init__(self):\n","        self.name = 'masked_loss'\n","        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","    def __call__(self, y_true, y_pred):\n","        # Calculate the loss for each item in the batch.\n","        loss = self.loss(y_true, y_pred)\n","    \n","        # Mask off the losses on padding.\n","        mask = tf.cast(y_true != 0, tf.float32)\n","        loss *= mask\n","\n","        # Return the total.\n","        return tf.reduce_sum(loss)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"X-znrarH29Kd","executionInfo":{"status":"ok","timestamp":1643909455040,"user_tz":300,"elapsed":4,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["class TextSummarizerModel(Model):\n","    def __init__(self, use_tf_function=True):\n","        super(TextSummarizerModel, self).__init__()\n","        self.encoder = Encoder(vocab_size, embed_size)\n","        self.decoder = Decoder(vocab_size, embed_size, 128)\n","        self.use_tf_function = use_tf_function\n","\n","    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n","        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n","        # Run the decoder one step.\n","        decoder_input = DecoderInput(new_tokens=input_token,\n","                               enc_output=enc_output,\n","                               mask=input_mask)\n","        # print('Entered Decoder')\n","        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n","        # print('Exit Decoder')\n","        # `self.loss` returns the total for non-padded tokens\n","        y = target_token\n","        y_pred = dec_result.logits\n","        step_loss = self.loss(y, y_pred)\n","        # print('Exit Loop Decoder')\n","        return step_loss, dec_state\n","\n","    def get_masks(self, input_tokens, target_tokens):\n","        return (input_tokens != 0), (target_tokens != 0)\n","\n","    def test_step(self, inputs):\n","        max_target_length = maxlen_summary\n","        X_eval, y_eval = inputs\n","        input_mask, target_mask = self.get_masks(X_eval, y_eval)\n","        enc_output, enc_state = self.encoder(X_eval)\n","        dec_state = enc_state\n","        loss = tf.constant(0.0)\n","        for t in range(max_target_length - 1):\n","            new_tokens = y_eval[:, t:t+2]\n","            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n","                                             enc_output, dec_state)\n","            loss = loss + step_loss\n","        \n","        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n","        return {'average_loss': average_loss}\n","\n","    def _train_step(self, input_tokens, target_tokens):\n","        max_target_length = maxlen_summary\n","\n","        steps_per_epoch = X_train.shape[0] // batch_size\n","\n","        for i in tf.range(steps_per_epoch):\n","\n","            batch_inputs = input_tokens[i * batch_size: (i + 1) * batch_size, :]\n","            batch_outputs = target_tokens[i * batch_size: (i + 1) * batch_size, :]\n","\n","            input_mask, target_mask = self.get_masks(batch_inputs, batch_outputs)\n","\n","            with tf.GradientTape() as tape:\n","                # Encode the input\n","                enc_output, enc_state = self.encoder(batch_inputs)\n","\n","                # Initialize the decoder's state to the encoder's final state.\n","                # This only works if the encoder and decoder have the same number of\n","                # units.\n","                dec_state = enc_state\n","                loss = tf.constant(0.0)\n","\n","                for t in tf.range(max_target_length - 1):\n","                    # Pass in two tokens from the target sequence:\n","                    # 1. The current input to the decoder.\n","                    # 2. The target for the decoder's next prediction.\n","                    new_tokens = batch_outputs[:, t:t+2]\n","                    step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n","                                             enc_output, dec_state)\n","                    loss = loss + step_loss\n","\n","                # Average the loss over all non padding tokens.\n","                average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n","\n","            # Apply an optimization step\n","            variables = self.trainable_variables\n","            gradients = tape.gradient(average_loss, variables)\n","            self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","            # Return a dict mapping metric names to current value\n","            # print('Batch {}: Loss = {}'.format(i, average_loss))\n","            # print('Time Taken for batch: {}s'.format(time.time() - start_time))\n","            return {'batch_loss': average_loss}\n","\n","    @tf.function\n","    def _tf_train_step(self, input_tokens, output_tokens):\n","        # print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","        return self._train_step(input_tokens, output_tokens)\n","\n","    def train_step(self, inputs):\n","        input_tokens, output_tokens = inputs\n","        if self.use_tf_function:\n","            return self._tf_train_step(input_tokens, output_tokens)\n","        else:\n","            return self._train_step(input_tokens, output_tokens)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"u2zSevmMxUKj","executionInfo":{"status":"ok","timestamp":1643909455040,"user_tz":300,"elapsed":3,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["class BatchLogs(tf.keras.callbacks.Callback):\n","    def __init__(self, key):\n","        self.key = key\n","        self.logs = []\n","\n","    def on_train_batch_end(self, n, logs):\n","        self.logs.append(logs[self.key])\n","\n","batch_loss = BatchLogs('batch_loss')"]},{"cell_type":"code","source":["opt_path = 'optimizer_weights_02-02-2022_06:_51:_38_22.npy'\n","optimizer_weights_path = filepath + opt_path"],"metadata":{"id":"gmd23l42v9OM","executionInfo":{"status":"ok","timestamp":1643909455040,"user_tz":300,"elapsed":3,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["optimizer_weights = np.load(optimizer_weights_path, allow_pickle = True)"],"metadata":{"id":"8Ry6-uLh4NNu","executionInfo":{"status":"ok","timestamp":1643909462210,"user_tz":300,"elapsed":7173,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","execution_count":24,"metadata":{"id":"cgA17jgWxEKn","executionInfo":{"status":"ok","timestamp":1643909464784,"user_tz":300,"elapsed":2594,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"outputs":[],"source":["model = TextSummarizerModel()"]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)"],"metadata":{"id":"iyrQIYJLvNgr","executionInfo":{"status":"ok","timestamp":1643909464785,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class OptimizerSaver(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        weights = optimizer.get_weights()\n","        optimizer_path = filepath + 'optimizer_weights_' + dt_string + '_{}.npy'.format(epoch)\n","        np.save(optimizer_path, optimizer.get_weights())\n","\n","op_saver = OptimizerSaver()"],"metadata":{"id":"B-yASsT1wW48","executionInfo":{"status":"ok","timestamp":1643909464785,"user_tz":300,"elapsed":5,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Configure the loss and optimizer\n","model.compile(optimizer = optimizer, loss = MaskedLoss())"],"metadata":{"id":"yJMQ0zAf33yH","executionInfo":{"status":"ok","timestamp":1643909464786,"user_tz":300,"elapsed":6,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["weights_path = 'retrained_model_checkpoint_weights_02-02-2022_06:_51:_38_23.h5'"],"metadata":{"id":"3yLse-583On1","executionInfo":{"status":"ok","timestamp":1643909464786,"user_tz":300,"elapsed":5,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model.fit(X_train, y_train)"],"metadata":{"id":"WtVd2ujq3a7h","colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"status":"error","timestamp":1643909650163,"user_tz":300,"elapsed":185139,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}},"outputId":"12f10414-3fa0-4a9d-dca3-f42a53e838cd"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":[" 139/2485 [>.............................] - ETA: 47:58 - batch_loss: 8.6976"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    947\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["optimizer.set_weights(optimizer_weights)"],"metadata":{"id":"OmW1gDKt4ICg","executionInfo":{"status":"ok","timestamp":1643909653995,"user_tz":300,"elapsed":1305,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer = optimizer, loss = MaskedLoss())"],"metadata":{"id":"Ct8V2Yu3Jvmp","executionInfo":{"status":"ok","timestamp":1643909653995,"user_tz":300,"elapsed":5,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["model.built = True"],"metadata":{"id":"yZAMEsi23taE","executionInfo":{"status":"ok","timestamp":1643909653996,"user_tz":300,"elapsed":5,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model.load_weights(filepath + weights_path)"],"metadata":{"id":"i9ks_Q0s3KEh","executionInfo":{"status":"ok","timestamp":1643909655823,"user_tz":300,"elapsed":1642,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","now = datetime.now()\n","dt_string = now.strftime(\"%d-%m-%Y_%H:_%M:_%S\")"],"metadata":{"id":"FMdEkE5bUWan","executionInfo":{"status":"ok","timestamp":1643909655823,"user_tz":300,"elapsed":3,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1643909656116,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"0V4NnoGS_mlt","outputId":"6d8a649c-8d71-41a8-f2b3-118b86524526"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]}],"source":["ckpt_filepath = filepath + 'retrained_model_checkpoint_weights_'+ dt_string +'_{epoch:02d}.h5'\n","callback = tf.keras.callbacks.ModelCheckpoint(\n","\tckpt_filepath,\n","\tperiod=1,\n","\tmonitor=\"batch_loss\",\n","\tverbose=0,\n","\tsave_best_only=False,\n","\tsave_weights_only=True,\n","\tmode=\"auto\",\n","\tsave_freq=\"epoch\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylVEgNeIy6gh","outputId":"26de81f7-383a-4110-f73f-969dd844ffce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","2485/2485 [==============================] - ETA: 0s - batch_loss: 2.7656"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order, subok=True)\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2485/2485 [==============================] - 3012s 1s/step - batch_loss: 2.7656\n","Epoch 2/25\n","2485/2485 [==============================] - 3018s 1s/step - batch_loss: 2.7378\n","Epoch 3/25\n","2485/2485 [==============================] - 3031s 1s/step - batch_loss: 2.7105\n","Epoch 4/25\n","2485/2485 [==============================] - 3062s 1s/step - batch_loss: 2.6841\n","Epoch 5/25\n","2485/2485 [==============================] - 3072s 1s/step - batch_loss: 2.6585\n","Epoch 6/25\n","2485/2485 [==============================] - 3048s 1s/step - batch_loss: 2.6330\n","Epoch 7/25\n","2485/2485 [==============================] - 3060s 1s/step - batch_loss: 2.6081\n","Epoch 8/25\n","2485/2485 [==============================] - 3041s 1s/step - batch_loss: 2.5837\n","Epoch 9/25\n","2485/2485 [==============================] - 3034s 1s/step - batch_loss: 2.5593\n","Epoch 10/25\n","2485/2485 [==============================] - 3049s 1s/step - batch_loss: 2.5361\n","Epoch 11/25\n","2485/2485 [==============================] - 3039s 1s/step - batch_loss: 2.5141\n","Epoch 12/25\n","2485/2485 [==============================] - 3053s 1s/step - batch_loss: 2.4910\n","Epoch 13/25\n","2485/2485 [==============================] - 3070s 1s/step - batch_loss: 2.4695\n","Epoch 14/25\n","1431/2485 [================>.............] - ETA: 21:34 - batch_loss: 2.4274"]}],"source":["model.fit(X_train, y_train, epochs = 25, callbacks=[batch_loss, callback, op_saver])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCFuMdAHM56C"},"outputs":[],"source":["model.save_weights(filepath + 'retrained_final_model_weights_' + dt_string + '.h5')"]},{"cell_type":"code","source":["np.save(filepath + 'final_optimizer_weights_' + dt_string + '.npy', optimizer.get_weights())"],"metadata":{"id":"eCINnnZJQ4YP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fx8-h6tayIv1"},"source":["### Model Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Xla7hMDVnPb"},"outputs":[],"source":["class Summarizer(tf.Module):\n","    def __init__(self, encoder, decoder):\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.output_token_string_from_index = (tf.keras.layers.StringLookup(\n","            vocabulary = list(word_index.keys()),\n","            mask_token='',\n","            invert=True))\n","        # The output should never generate padding, unknown, or start.\n","        index_from_string = tf.keras.layers.StringLookup(vocabulary = list(word_index.keys()), mask_token='')\n","        token_mask_ids = index_from_string(['', '<s>', '</s>']).numpy()\n","        token_mask = np.zeros([vocab_size], dtype=np.bool)\n","        token_mask[np.array(token_mask_ids)] = True\n","        self.token_mask = token_mask\n","        self.start_token = index_from_string(tf.constant('<s>'))\n","        self.end_token = index_from_string(tf.constant('</s>'))\n","    \n","    def tokens_to_text(self, result_tokens):\n","        result_text_tokens = self.output_token_string_from_index(result_tokens)\n","        \n","        result_text = tf.strings.reduce_join(result_text_tokens,\n","                                       axis=1, separator=' ')\n","\n","        result_text = tf.strings.strip(result_text)\n","        return result_text\n","\n","    def sample(self, logits, temperature):\n","        token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n","  \n","        # Set the logits for all masked tokens to -inf, so they are never chosen.\n","        logits = tf.where(self.token_mask, -np.inf, logits)\n","\n","        if temperature == 0.0:\n","            new_tokens = tf.argmax(logits, axis=-1)\n","        else: \n","            logits = tf.squeeze(logits, axis=1)\n","        \n","        new_tokens = tf.random.categorical(logits/temperature,\n","                                        num_samples=1)\n","        return new_tokens\n","\n","    def abstractive_summarize(self, input_tokens, *, max_length=maxlen_article, return_attention=True, temperature=1.0):\n","        batch_size = tf.shape(input_tokens)[0]\n","        enc_output, enc_state = self.encoder(input_tokens)\n","\n","        dec_state = enc_state\n","        new_tokens = tf.fill([batch_size, 1], self.start_token)\n","\n","        result_tokens = []\n","        attention = []\n","        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","\n","        for _ in range(max_length):\n","            dec_input = DecoderInput(new_tokens=new_tokens,\n","                             enc_output=enc_output,\n","                             mask=(input_tokens!=0))\n","\n","            dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n","\n","            attention.append(dec_result.attention_weights)\n","\n","            new_tokens = self.sample(dec_result.logits, temperature)\n","\n","            # If a sequence produces an `end_token`, set it `done`\n","            done = done | (new_tokens == self.end_token)\n","            # Once a sequence is done it only produces 0-padding.\n","            new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n","\n","            # Collect the generated tokens\n","            result_tokens.append(new_tokens)\n","\n","            if tf.executing_eagerly() and tf.reduce_all(done):\n","                break\n","\n","        # Convert the list of generates token ids to a list of strings.\n","        result_tokens = tf.concat(result_tokens, axis=-1)\n","        result_text = self.tokens_to_text(result_tokens)\n","\n","        if return_attention:\n","            attention_stack = tf.concat(attention, axis=1)\n","            return {'text': result_text, 'attention': attention_stack}\n","        else:\n","            return {'text': result_text}\n","\n","    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.int32, shape=[None, maxlen_article])])\n","    def tf_summarize(self, input_tokens):\n","        return self.abstractive_summarize(input_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSlPvCpBW-q6"},"outputs":[],"source":["summarizer = Summarizer(encoder = model.encoder, decoder = model.decoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"error","timestamp":1642773313426,"user":{"displayName":"Sarvesh Relekar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07404693667092897113"},"user_tz":300},"id":"AzeHqc2wZ7as","outputId":"4dacf6e7-ace7-46ef-9619-8881ee5d02f9"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-119-e39eb835cf98>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    signatures={'serving_default': summarizer.tf_summarize}\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}],"source":["tf.saved_model.save(summarizer, filepath + 'summarizer',\n","                    signatures={'serving_default': summarizer.tf_summarize})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcbO733GyA_K"},"outputs":[],"source":["summarizer.tf_summarize(X_val[:5, :])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"CNN DailyMail Text Summarization (No GLoVe + Bahdanau only) Retraining (No Preprocessing).ipynb","provenance":[{"file_id":"1LInF9hNZ3P9N34t-GaeTlTJaKkmEY066","timestamp":1643007127117},{"file_id":"1xG5KgRJexbadT6TxxyHBMMC6yJLKTAgT","timestamp":1642996912294},{"file_id":"1HMuuHfV8JxPFmKisiZtS2Tawkuo-IWJz","timestamp":1642625536164},{"file_id":"1pX4fzQK21KnhNwRLS-SZsHsrplwvchzK","timestamp":1642524248869}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}